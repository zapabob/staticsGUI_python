#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Professional Statistics Suite - Hardware & AI Configuration Management
ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«çµ±è¨ˆã‚¹ã‚¤ãƒ¼ãƒˆ - ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ»AIè¨­å®šç®¡ç†

RTX 30/40/50 Series & Apple Silicon M2+ Optimized
"""

import os
import platform
import subprocess
import sys
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple
import json
import yaml
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

class HardwareDetector:
    """ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ¤œå‡ºãƒ»æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.platform = platform.system()
        self.architecture = platform.machine()
        self.python_version = platform.python_version()
        
        # Hardware information
        self.gpu_info = self._detect_gpu()
        self.cpu_info = self._detect_cpu()
        self.memory_info = self._detect_memory()
        
        # Optimization settings
        self.optimal_settings = self._determine_optimal_settings()
    
    def _detect_gpu(self) -> Dict[str, Any]:
        """GPUæ¤œå‡ºï¼ˆNVIDIA RTX 30/40/50, Apple Silicon, Intel, AMDï¼‰"""
        gpu_info = {
            'nvidia': {'available': False, 'devices': [], 'cuda_version': None},
            'apple_metal': {'available': False, 'devices': []},
            'intel': {'available': False, 'devices': []},
            'amd': {'available': False, 'devices': []}
        }
        
        # NVIDIA GPU detection
        try:
            import torch
            if torch.cuda.is_available():
                gpu_info['nvidia']['available'] = True
                gpu_info['nvidia']['cuda_version'] = torch.version.cuda
                
                for i in range(torch.cuda.device_count()):
                    device_props = torch.cuda.get_device_properties(i)
                    gpu_info['nvidia']['devices'].append({
                        'name': device_props.name,
                        'compute_capability': f"{device_props.major}.{device_props.minor}",
                        'memory_gb': device_props.total_memory / (1024**3),
                        'multiprocessor_count': device_props.multi_processor_count,
                        'is_rtx_30_series': 'RTX 30' in device_props.name or 'RTX 31' in device_props.name or 'RTX 32' in device_props.name,
                        'is_rtx_40_series': 'RTX 40' in device_props.name or 'RTX 41' in device_props.name,
                        'is_rtx_50_series': 'RTX 50' in device_props.name or 'RTX 51' in device_props.name,
                        'optimization_level': self._get_nvidia_optimization_level(device_props.name)
                    })
        except ImportError:
            pass
        
        # Apple Metal detection (M1, M2, M3+ chips)
        if self.platform == "Darwin":
            try:
                # Check for Apple Silicon
                if self.architecture in ['arm64', 'aarch64']:
                    gpu_info['apple_metal']['available'] = True
                    chip_info = self._detect_apple_chip()
                    gpu_info['apple_metal']['devices'].append(chip_info)
            except Exception:
                pass
        
        return gpu_info
    
    def _detect_apple_chip(self) -> Dict[str, Any]:
        """Apple Silicon ãƒãƒƒãƒ—æ¤œå‡º"""
        try:
            # Get chip information using system_profiler
            result = subprocess.run(['system_profiler', 'SPHardwareDataType'], 
                                   capture_output=True, text=True)
            output = result.stdout
            
            chip_name = "Unknown Apple Silicon"
            performance_cores = 0
            efficiency_cores = 0
            neural_engine = False
            
            if "Apple M" in output:
                lines = output.split('\n')
                for line in lines:
                    if "Chip:" in line:
                        chip_name = line.split(":")[-1].strip()
                    elif "Total Number of Cores:" in line:
                        cores_info = line.split(":")[-1].strip()
                        if "(" in cores_info:
                            # Parse format like "8 (4 performance and 4 efficiency)"
                            total_cores = int(cores_info.split()[0])
                            if "performance" in cores_info and "efficiency" in cores_info:
                                parts = cores_info.split("(")[1].split(")")[0]
                                perf_part = [p for p in parts.split(" and ") if "performance" in p][0]
                                eff_part = [p for p in parts.split(" and ") if "efficiency" in p][0]
                                performance_cores = int(perf_part.split()[0])
                                efficiency_cores = int(eff_part.split()[0])
            
            # Determine optimization level based on chip
            optimization_level = "standard"
            if "M2" in chip_name or "M3" in chip_name:
                optimization_level = "high"
                neural_engine = True
            elif "M1" in chip_name:
                optimization_level = "medium"
                neural_engine = True
            
            return {
                'name': chip_name,
                'performance_cores': performance_cores,
                'efficiency_cores': efficiency_cores,
                'neural_engine': neural_engine,
                'optimization_level': optimization_level,
                'metal_support': True,
                'mlx_compatible': "M2" in chip_name or "M3" in chip_name
            }
            
        except Exception:
            return {
                'name': 'Apple Silicon (Unknown)',
                'optimization_level': 'medium',
                'metal_support': True,
                'mlx_compatible': False
            }
    
    def _get_nvidia_optimization_level(self, gpu_name: str) -> str:
        """NVIDIA GPUæœ€é©åŒ–ãƒ¬ãƒ™ãƒ«æ±ºå®š"""
        if any(series in gpu_name for series in ['RTX 50', 'RTX 51']):
            return "maximum"  # RTX 50 series
        elif any(series in gpu_name for series in ['RTX 40', 'RTX 41']):
            return "high"     # RTX 40 series
        elif any(series in gpu_name for series in ['RTX 30', 'RTX 31', 'RTX 32']):
            return "high"     # RTX 30 series
        elif any(series in gpu_name for series in ['RTX 20', 'GTX 16']):
            return "medium"   # RTX 20/GTX 16 series
        else:
            return "standard" # Other GPUs
    
    def _detect_cpu(self) -> Dict[str, Any]:
        """CPUæƒ…å ±æ¤œå‡º"""
        cpu_info = {
            'cores': os.cpu_count(),
            'architecture': self.architecture,
            'platform': self.platform
        }
        
        try:
            import psutil
            cpu_info['frequency_mhz'] = psutil.cpu_freq().max if psutil.cpu_freq() else 0
            cpu_info['physical_cores'] = psutil.cpu_count(logical=False)
            cpu_info['logical_cores'] = psutil.cpu_count(logical=True)
        except ImportError:
            pass
        
        return cpu_info
    
    def _detect_memory(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªæƒ…å ±æ¤œå‡º"""
        memory_info = {'total_gb': 0, 'available_gb': 0}
        
        try:
            import psutil
            memory = psutil.virtual_memory()
            memory_info = {
                'total_gb': memory.total / (1024**3),
                'available_gb': memory.available / (1024**3),
                'percent_used': memory.percent
            }
        except ImportError:
            pass
        
        return memory_info
    
    def _determine_optimal_settings(self) -> Dict[str, Any]:
        """æœ€é©è¨­å®šæ±ºå®š"""
        settings = {
            'framework': 'auto',
            'device': 'auto',
            'precision': 'float32',
            'batch_size': 'auto',
            'num_workers': 'auto',
            'memory_fraction': 0.8,
            'optimization_level': 'medium'
        }
        
        # GPU based optimization
        if self.gpu_info['nvidia']['available']:
            nvidia_devices = self.gpu_info['nvidia']['devices']
            if nvidia_devices:
                best_device = max(nvidia_devices, key=lambda x: x['memory_gb'])
                settings['framework'] = 'pytorch_cuda'
                settings['device'] = 'cuda'
                settings['optimization_level'] = best_device['optimization_level']
                
                # RTX 40/50 series optimizations
                if best_device['is_rtx_40_series'] or best_device['is_rtx_50_series']:
                    settings['precision'] = 'mixed'  # Use mixed precision
                    settings['memory_fraction'] = 0.9
                
        elif self.gpu_info['apple_metal']['available']:
            apple_device = self.gpu_info['apple_metal']['devices'][0]
            settings['framework'] = 'tensorflow_metal'
            settings['device'] = 'mps'  # Metal Performance Shaders
            settings['optimization_level'] = apple_device['optimization_level']
            
            # M2+ optimizations
            if apple_device.get('mlx_compatible', False):
                settings['framework'] = 'mlx'
                settings['precision'] = 'float16'
        
        # CPU optimizations
        cpu_cores = self.cpu_info.get('logical_cores', os.cpu_count())
        settings['num_workers'] = min(cpu_cores - 1, 8)  # Leave one core for system
        
        # Memory optimizations
        total_memory = self.memory_info.get('total_gb', 8)
        if total_memory >= 32:
            settings['batch_size'] = 'large'
        elif total_memory >= 16:
            settings['batch_size'] = 'medium'
        else:
            settings['batch_size'] = 'small'
        
        return settings
    
    def get_optimization_recommendations(self) -> List[str]:
        """æœ€é©åŒ–æ¨å¥¨äº‹é …å–å¾—"""
        recommendations = []
        
        # GPU recommendations
        if self.gpu_info['nvidia']['available']:
            nvidia_devices = self.gpu_info['nvidia']['devices']
            for device in nvidia_devices:
                if device['is_rtx_40_series'] or device['is_rtx_50_series']:
                    recommendations.append(f"ğŸš€ {device['name']} detected: Use mixed precision training for maximum performance")
                    recommendations.append("ğŸ’¡ Enable CUDA memory optimization for large datasets")
                elif device['is_rtx_30_series']:
                    recommendations.append(f"âš¡ {device['name']} detected: Optimize batch size for RTX 30 series")
        
        elif self.gpu_info['apple_metal']['available']:
            apple_device = self.gpu_info['apple_metal']['devices'][0]
            if apple_device.get('mlx_compatible', False):
                recommendations.append(f"ğŸ {apple_device['name']} detected: Use MLX framework for native acceleration")
                recommendations.append("ğŸ”¥ Enable Metal Performance Shaders for optimal performance")
            else:
                recommendations.append("ğŸ Apple Silicon detected: Use TensorFlow Metal backend")
        
        # Memory recommendations
        total_memory = self.memory_info.get('total_gb', 0)
        if total_memory < 16:
            recommendations.append("âš ï¸ Consider upgrading to 16GB+ RAM for large dataset processing")
        elif total_memory >= 32:
            recommendations.append("ğŸ’ª Sufficient RAM detected: Enable large batch processing")
        
        return recommendations

class AIConfig:
    """AI APIè¨­å®šç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–å¯¾å¿œï¼‰"""
    
    def __init__(self):
        self.config_dir = Path("config")
        self.config_dir.mkdir(exist_ok=True)
        
        # Hardware detection
        self.hardware = HardwareDetector()
        
        # APIè¨­å®š
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
        
        # MCPè¨­å®š
        self.mcp_server_url = os.getenv("MCP_SERVER_URL", "ws://localhost:8080")
        self.mcp_enabled = os.getenv("MCP_ENABLED", "false").lower() == "true"
        
        # AIæ©Ÿèƒ½è¨­å®š
        self.enable_vision = os.getenv("ENABLE_VISION", "true").lower() == "true"
        self.enable_nlp = os.getenv("ENABLE_NLP", "true").lower() == "true"
        self.default_model = os.getenv("DEFAULT_MODEL", "gpt-4-vision-preview")
        
        # ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–è¨­å®š
        self.gpu_acceleration = self.hardware.gpu_info['nvidia']['available'] or self.hardware.gpu_info['apple_metal']['available']
        self.optimal_framework = self.hardware.optimal_settings['framework']
        self.optimal_device = self.hardware.optimal_settings['device']
        self.optimal_precision = self.hardware.optimal_settings['precision']
        
        # ç”»åƒå‡¦ç†è¨­å®š
        self.max_image_size = int(os.getenv("MAX_IMAGE_SIZE", "10485760"))  # 10MB
        self.supported_formats = ["jpg", "jpeg", "png", "gif", "bmp", "tiff", "webp"]
        
        # OCRè¨­å®š
        self.tesseract_cmd = os.getenv("TESSERACT_CMD")
        self.ocr_languages = os.getenv("OCR_LANGUAGES", "eng+jpn").split("+")
        
        self._load_config()
    
    def _load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        config_file = self.config_dir / "ai_config.yaml"
        if config_file.exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    self._update_from_config(config)
            except Exception as e:
                print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _update_from_config(self, config: Dict[str, Any]):
        """è¨­å®šè¾æ›¸ã‹ã‚‰æ›´æ–°"""
        for key, value in config.items():
            if hasattr(self, key):
                setattr(self, key, value)
    
    def save_config(self):
        """è¨­å®šã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        config = {
            'mcp_server_url': self.mcp_server_url,
            'mcp_enabled': self.mcp_enabled,
            'enable_vision': self.enable_vision,
            'enable_nlp': self.enable_nlp,
            'default_model': self.default_model,
            'max_image_size': self.max_image_size,
            'supported_formats': self.supported_formats,
            'ocr_languages': self.ocr_languages,
            'hardware_info': {
                'gpu_info': self.hardware.gpu_info,
                'cpu_info': self.hardware.cpu_info,
                'memory_info': self.hardware.memory_info,
                'optimal_settings': self.hardware.optimal_settings
            }
        }
        
        config_file = self.config_dir / "ai_config.yaml"
        try:
            with open(config_file, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        except Exception as e:
            print(f"è¨­å®šä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_hardware_status(self) -> Dict[str, Any]:
        """ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢çŠ¶æ…‹å–å¾—"""
        return {
            'platform': self.hardware.platform,
            'architecture': self.hardware.architecture,
            'gpu_acceleration': self.gpu_acceleration,
            'optimal_framework': self.optimal_framework,
            'optimal_device': self.optimal_device,
            'gpu_devices': self.hardware.gpu_info,
            'cpu_cores': self.hardware.cpu_info.get('logical_cores', 'Unknown'),
            'total_memory_gb': round(self.hardware.memory_info.get('total_gb', 0), 1),
            'recommendations': self.hardware.get_optimization_recommendations()
        }
    
    def is_api_configured(self, provider: str) -> bool:
        """APIè¨­å®šç¢ºèª"""
        if provider.lower() == "openai":
            return bool(self.openai_api_key)
        elif provider.lower() == "google":
            return bool(self.google_api_key)
        elif provider.lower() == "anthropic":
            return bool(self.anthropic_api_key)
        return False
    
    def get_available_providers(self) -> list:
        """åˆ©ç”¨å¯èƒ½ãªAPIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä¸€è¦§"""
        providers = []
        if self.is_api_configured("openai"):
            providers.append("OpenAI")
        if self.is_api_configured("google"):
            providers.append("Google")
        if self.is_api_configured("anthropic"):
            providers.append("Anthropic")
        return providers

# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
ai_config = AIConfig()

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–å¯¾å¿œï¼‰
STATISTICAL_ANALYSIS_PROMPTS = {
    'hardware_optimized_analysis': """
ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã€åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«æœ€é©åŒ–ã•ã‚ŒãŸåˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±:
- GPU: {gpu_info}
- æ¨å¥¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯: {framework}
- æœ€é©åŒ–ãƒ¬ãƒ™ãƒ«: {optimization_level}

ãƒ‡ãƒ¼ã‚¿æƒ…å ±:
- å½¢çŠ¶: {shape}
- åˆ—å: {columns}
- ãƒ‡ãƒ¼ã‚¿å‹: {dtypes}

åˆ†æè¦æ±‚: {analysis_request}

ä»¥ä¸‹ã‚’å«ã‚€æœ€é©åŒ–ã•ã‚ŒãŸPythonã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š
1. ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ¤œå‡ºã¨è¨­å®š
2. æœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å‡¦ç†
3. GPU/Apple SiliconåŠ é€Ÿã®æ´»ç”¨
4. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå‡¦ç†
""",
    
    'data_description': """
ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦åˆ†æã—ã¦ãã ã•ã„ï¼š

ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {shape}
åˆ—å: {columns}
ãƒ‡ãƒ¼ã‚¿å‹: {dtypes}
æ¬ æå€¤: {missing_values}

ä»¥ä¸‹ã®çµ±è¨ˆè§£æã‚’å®Ÿè¡Œã—ã€çµæœã‚’Pythonã‚³ãƒ¼ãƒ‰ã¨ã—ã¦è¿”ã—ã¦ãã ã•ã„ï¼š
{analysis_request}

å›ç­”ã¯ä»¥ä¸‹ã®å½¢å¼ã§ï¼š
1. åˆ†æã®èª¬æ˜
2. å®Ÿè¡Œã™ã‚‹Pythonã‚³ãƒ¼ãƒ‰
3. çµæœã®è§£é‡ˆ
""",
    
    'gpu_accelerated_ml': """
GPUåŠ é€Ÿã‚’æ´»ç”¨ã—ãŸæ©Ÿæ¢°å­¦ç¿’åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

åˆ©ç”¨å¯èƒ½GPU: {gpu_devices}
æ¨å¥¨è¨­å®š: {optimal_settings}

ãƒ‡ãƒ¼ã‚¿: {data_info}
åˆ†æç›®æ¨™: {objective}

ä»¥ä¸‹ã‚’å«ã‚€ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆï¼š
1. GPUæ¤œå‡ºã¨åˆæœŸåŒ–
2. æœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
3. GPUåŠ é€Ÿãƒ¢ãƒ‡ãƒ«è¨“ç·´
4. çµæœã®å¯è¦–åŒ–
"""
}

# ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨­å®š
HARDWARE_OPTIMIZED_CONFIGS = {
    'nvidia_rtx_50': {
        'precision': 'mixed',
        'batch_size_multiplier': 2.0,
        'memory_fraction': 0.95,
        'optimization_flags': ['--enable-tensor-cores', '--enable-flash-attention']
    },
    'nvidia_rtx_40': {
        'precision': 'mixed',
        'batch_size_multiplier': 1.8,
        'memory_fraction': 0.9,
        'optimization_flags': ['--enable-tensor-cores']
    },
    'nvidia_rtx_30': {
        'precision': 'float32',
        'batch_size_multiplier': 1.5,
        'memory_fraction': 0.85,
        'optimization_flags': ['--enable-tensor-cores']
    },
    'apple_m3': {
        'framework': 'mlx',
        'precision': 'float16',
        'batch_size_multiplier': 1.2,
        'metal_optimization': True
    },
    'apple_m2': {
        'framework': 'tensorflow_metal',
        'precision': 'float32',
        'batch_size_multiplier': 1.0,
        'metal_optimization': True
    }
}

# ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å¯¾å¿œï¼‰
MODEL_CONFIGS = {
    'openai': {
        'gpt-4-vision-preview': {
            'supports_vision': True,
            'max_tokens': 4096,
            'temperature': 0.1
        },
        'gpt-4': {
            'supports_vision': False,
            'max_tokens': 8192,
            'temperature': 0.1
        },
        'gpt-3.5-turbo': {
            'supports_vision': False,
            'max_tokens': 4096,
            'temperature': 0.1
        }
    },
    'google': {
        'gemini-pro-vision': {
            'supports_vision': True,
            'max_tokens': 2048,
            'temperature': 0.1
        },
        'gemini-pro': {
            'supports_vision': False,
            'max_tokens': 2048,
            'temperature': 0.1
        }
    }
}

def get_hardware_summary() -> str:
    """ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ¦‚è¦å–å¾—"""
    hardware = ai_config.hardware
    summary = f"ğŸ–¥ï¸ Platform: {hardware.platform} ({hardware.architecture})\n"
    
    if hardware.gpu_info['nvidia']['available']:
        for device in hardware.gpu_info['nvidia']['devices']:
            summary += f"ğŸ® NVIDIA: {device['name']} ({device['memory_gb']:.1f}GB)\n"
    
    if hardware.gpu_info['apple_metal']['available']:
        device = hardware.gpu_info['apple_metal']['devices'][0]
        summary += f"ğŸ Apple Silicon: {device['name']}\n"
    
    summary += f"ğŸ§  CPU: {hardware.cpu_info.get('logical_cores', 'Unknown')} cores\n"
    summary += f"ğŸ’¾ Memory: {hardware.memory_info.get('total_gb', 0):.1f}GB\n"
    
    return summary 