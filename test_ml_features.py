#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ©Ÿæ¢°å­¦ç¿’ãƒ»æ·±å±¤å­¦ç¿’æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import pandas as pd
import numpy as np
from pathlib import Path

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import lightgbm as lgb

# æ·±å±¤å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    TORCH_AVAILABLE = True
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”¥ PyTorch: Available | Device: {DEVICE}")
except ImportError:
    TORCH_AVAILABLE = False
    print("âŒ PyTorch: Not Available")

def test_ml_algorithms():
    """æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ¤– æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_path = Path("data/machine_learning_sample.csv")
    if not data_path.exists():
        print("âŒ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«sample_data.pyã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return
    
    df = pd.read_csv(data_path)
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df.shape}")
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    feature_cols = [col for col in df.columns if col.startswith('feature_')]
    X = df[feature_cols]
    y = df['target']
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ†ã‚¹ãƒˆ
    algorithms = {
        "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
        "XGBoost": xgb.XGBClassifier(random_state=42, eval_metric='logloss'),
        "LightGBM": lgb.LGBMClassifier(random_state=42, verbose=-1)
    }
    
    results = {}
    
    for name, model in algorithms.items():
        print(f"\nğŸ”§ {name} ãƒ†ã‚¹ãƒˆä¸­...")
        
        # å­¦ç¿’
        model.fit(X_train, y_train)
        
        # äºˆæ¸¬
        y_pred = model.predict(X_test)
        
        # è©•ä¾¡
        accuracy = accuracy_score(y_test, y_pred)
        results[name] = accuracy
        
        print(f"âœ… {name} Accuracy: {accuracy:.4f}")
        
        # ç‰¹å¾´é‡é‡è¦åº¦
        if hasattr(model, 'feature_importances_'):
            importance = model.feature_importances_
            top_features = sorted(zip(feature_cols, importance), key=lambda x: x[1], reverse=True)[:3]
            print(f"   Top 3 features: {[f'{feat}: {imp:.3f}' for feat, imp in top_features]}")
    
    # çµæœã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“ˆ æ©Ÿæ¢°å­¦ç¿’ãƒ†ã‚¹ãƒˆçµæœ:")
    for name, acc in results.items():
        print(f"â€¢ {name}: {acc:.4f}")
    
    best_model = max(results, key=results.get)
    print(f"ğŸ† æœ€é«˜æ€§èƒ½: {best_model} ({results[best_model]:.4f})")

def test_deep_learning():
    """æ·±å±¤å­¦ç¿’ãƒ†ã‚¹ãƒˆ"""
    if not TORCH_AVAILABLE:
        print("\nâŒ PyTorchãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€æ·±å±¤å­¦ç¿’ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
    
    print("\nğŸ§  æ·±å±¤å­¦ç¿’ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æº–å‚™
    data_path = Path("data/machine_learning_sample.csv")
    df = pd.read_csv(data_path)
    
    feature_cols = [col for col in df.columns if col.startswith('feature_')]
    X = df[feature_cols].values
    y = df['target'].values
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # PyTorchãƒ†ãƒ³ã‚µãƒ¼ã«å¤‰æ›
    X_train_tensor = torch.FloatTensor(X_train_scaled).to(DEVICE)
    X_test_tensor = torch.FloatTensor(X_test_scaled).to(DEVICE)
    y_train_tensor = torch.LongTensor(y_train).to(DEVICE)
    y_test_tensor = torch.LongTensor(y_test).to(DEVICE)
    
    print(f"ğŸ“± ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {DEVICE}")
    print(f"ğŸ¯ ã‚¯ãƒ©ã‚¹æ•°: {len(np.unique(y))}")
    print(f"ğŸ“Š ç‰¹å¾´é‡æ•°: {X_train.shape[1]}")
    
    # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å®šç¾©
    class SimpleNN(nn.Module):
        def __init__(self, input_size, hidden_size, num_classes):
            super(SimpleNN, self).__init__()
            self.fc1 = nn.Linear(input_size, hidden_size)
            self.fc2 = nn.Linear(hidden_size, hidden_size//2)
            self.fc3 = nn.Linear(hidden_size//2, num_classes)
            self.relu = nn.ReLU()
            self.dropout = nn.Dropout(0.2)
            self.batch_norm1 = nn.BatchNorm1d(hidden_size)
            self.batch_norm2 = nn.BatchNorm1d(hidden_size//2)
            
        def forward(self, x):
            x = self.relu(self.batch_norm1(self.fc1(x)))
            x = self.dropout(x)
            x = self.relu(self.batch_norm2(self.fc2(x)))
            x = self.dropout(x)
            x = self.fc3(x)
            return x
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    input_size = X_train.shape[1]
    hidden_size = 128
    num_classes = len(np.unique(y))
    
    model = SimpleNN(input_size, hidden_size, num_classes).to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    print(f"ğŸ—ï¸ ãƒ¢ãƒ‡ãƒ«æ§‹é€ : {input_size} â†’ {hidden_size} â†’ {hidden_size//2} â†’ {num_classes}")
    
    # å­¦ç¿’
    model.train()
    epochs = 50
    print(f"ğŸ“ å­¦ç¿’é–‹å§‹: {epochs} epochs")
    
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_train_tensor)
        loss = criterion(outputs, y_train_tensor)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 10 == 0:
            print(f"   Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")
    
    # è©•ä¾¡
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test_tensor)
        _, predicted = torch.max(test_outputs.data, 1)
        
    y_pred = predicted.cpu().numpy()
    y_test_np = y_test_tensor.cpu().numpy()
    
    accuracy = accuracy_score(y_test_np, y_pred)
    print(f"\nğŸ¯ æ·±å±¤å­¦ç¿’çµæœ:")
    print(f"â€¢ Accuracy: {accuracy:.4f}")
    print(f"â€¢ Device: {DEVICE}")
    print(f"â€¢ Model Parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # GPUä½¿ç”¨çŠ¶æ³ï¼ˆCUDAåˆ©ç”¨æ™‚ï¼‰
    if torch.cuda.is_available():
        print(f"â€¢ GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
        print(f"â€¢ GPU Memory Cached: {torch.cuda.memory_reserved() / 1024**2:.1f} MB")

def test_advanced_features():
    """é«˜åº¦ãªæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
    print("\nâš¡ é«˜åº¦ãªæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    # GPUçŠ¶æ³
    if torch.cuda.is_available():
        print(f"âœ… CUDA Available: {torch.cuda.is_available()}")
        print(f"âœ… GPU Count: {torch.cuda.device_count()}")
        print(f"âœ… Current Device: {torch.cuda.current_device()}")
        print(f"âœ… Device Name: {torch.cuda.get_device_name(0)}")
        print(f"âœ… CUDA Version: {torch.version.cuda}")
    else:
        print("âŒ CUDA not available - using CPU")
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
    import psutil
    memory_info = psutil.virtual_memory()
    print(f"\nğŸ’¾ ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª:")
    print(f"â€¢ Total: {memory_info.total / 1024**3:.1f} GB")
    print(f"â€¢ Available: {memory_info.available / 1024**3:.1f} GB")
    print(f"â€¢ Used: {memory_info.percent:.1f}%")
    
    # ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒª
    process = psutil.Process()
    process_memory = process.memory_info()
    print(f"\nğŸ“Š ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒª:")
    print(f"â€¢ RSS: {process_memory.rss / 1024**2:.1f} MB")
    print(f"â€¢ VMS: {process_memory.vms / 1024**2:.1f} MB")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ”¬ Professional Statistical Analysis Software")
    print("ğŸ¤– Machine Learning & Deep Learning Test Suite")
    print("=" * 60)
    
    # åŸºæœ¬æƒ…å ±
    print(f"Python Version: {'.'.join(map(str, __import__('sys').version_info[:3]))}")
    print(f"NumPy Version: {np.__version__}")
    print(f"Pandas Version: {pd.__version__}")
    
    if TORCH_AVAILABLE:
        print(f"PyTorch Version: {torch.__version__}")
        print(f"CUDA Available: {torch.cuda.is_available()}")
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    try:
        test_ml_algorithms()
        test_deep_learning()
        test_advanced_features()
        
        print(f"\nğŸ‰ å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("ğŸ“± çµ±è¨ˆã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã§ã€ŒğŸ¤– Machine Learningã€ãƒœã‚¿ãƒ³ã‚’è©¦ã—ã¦ãã ã•ã„ï¼")
        
    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 