#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Parallel Processing and Optimization Module
ä¸¦åˆ—å‡¦ç†ãƒ»æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

Author: Ryo Minegishi
License: MIT
"""

import numpy as np
import pandas as pd
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import multiprocessing as mp
from functools import partial, wraps
import time
import psutil
import gc
from typing import Callable, List, Dict, Any, Optional, Tuple
import warnings
from tqdm import tqdm
import threading
import queue
import asyncio
try:
    import numba
    from numba import jit, prange
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False
import joblib
from joblib import Parallel, delayed
from datetime import datetime
import os
import signal
import sys

# ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«æ©Ÿèƒ½ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from professional_utils import professional_logger, performance_monitor
    PROFESSIONAL_LOGGING = True
except ImportError:
    PROFESSIONAL_LOGGING = False

warnings.filterwarnings('ignore')

class ParallelProcessor:
    """ä¸¦åˆ—å‡¦ç†ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
    
    def __init__(self, n_jobs: int = -1, backend: str = 'threading'):
        self.n_jobs = n_jobs if n_jobs != -1 else mp.cpu_count()
        self.backend = backend  # 'threading', 'multiprocessing'
        self.max_memory_usage = 0.8  # æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—
        self.system_info = self._get_system_info()
        
        if PROFESSIONAL_LOGGING:
            professional_logger.info("ä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–", 
                                   n_jobs=self.n_jobs, 
                                   backend=backend,
                                   **self.system_info)
    
    def _get_system_info(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—"""
        try:
            cpu_freq = psutil.cpu_freq()
            cpu_freq_max = cpu_freq.max if cpu_freq else None
        except:
            cpu_freq_max = None
            
        return {
            'cpu_count': mp.cpu_count(),
            'memory_total_gb': psutil.virtual_memory().total / (1024**3),
            'memory_available_gb': psutil.virtual_memory().available / (1024**3),
            'cpu_freq_max': cpu_freq_max
        }
    
    def parallel_analysis(self, func: Callable, data_chunks: List[Any], 
                         *args, **kwargs) -> List[Any]:
        """ä¸¦åˆ—è§£æå®Ÿè¡Œ"""
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
        if not self._check_memory_availability():
            if PROFESSIONAL_LOGGING:
                professional_logger.warning("ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚ä¸¦åˆ—å‡¦ç†ã‚’åˆ¶é™ã—ã¾ã™")
            self.n_jobs = min(self.n_jobs, 2)
        
        results = []
        
        if self.backend == 'threading':
            results = self._thread_parallel(func, data_chunks, *args, **kwargs)
        elif self.backend == 'multiprocessing':
            results = self._process_parallel(func, data_chunks, *args, **kwargs)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†
            results = [func(chunk, *args, **kwargs) for chunk in data_chunks]
        
        return results
    
    def _thread_parallel(self, func: Callable, data_chunks: List[Any], 
                        *args, **kwargs) -> List[Any]:
        """ã‚¹ãƒ¬ãƒƒãƒ‰ä¸¦åˆ—å‡¦ç†"""
        with ThreadPoolExecutor(max_workers=self.n_jobs) as executor:
            futures = [executor.submit(func, chunk, *args, **kwargs) 
                      for chunk in data_chunks]
            
            results = []
            for future in tqdm(as_completed(futures), 
                             total=len(futures), 
                             desc="ğŸ”„ Thread Parallel Processing"):
                try:
                    result = future.result(timeout=300)  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    results.append(result)
                except Exception as e:
                    if PROFESSIONAL_LOGGING:
                        professional_logger.error(f"ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    results.append(None)
        
        return results
    
    def _process_parallel(self, func: Callable, data_chunks: List[Any], 
                         *args, **kwargs) -> List[Any]:
        """ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—å‡¦ç†"""
        try:
            with ProcessPoolExecutor(max_workers=self.n_jobs) as executor:
                futures = [executor.submit(func, chunk, *args, **kwargs) 
                          for chunk in data_chunks]
                
                results = []
                for future in tqdm(as_completed(futures), 
                                 total=len(futures), 
                                 desc="ğŸš€ Process Parallel Processing"):
                    try:
                        result = future.result(timeout=600)  # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                        results.append(result)
                    except Exception as e:
                        if PROFESSIONAL_LOGGING:
                            professional_logger.error(f"ãƒ—ãƒ­ã‚»ã‚¹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                        results.append(None)
            
            return results
        
        except Exception as e:
            if PROFESSIONAL_LOGGING:
                professional_logger.warning(f"ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—å‡¦ç†å¤±æ•—ã€ã‚¹ãƒ¬ãƒƒãƒ‰ä¸¦åˆ—ã«åˆ‡ã‚Šæ›¿ãˆ: {e}")
            return self._thread_parallel(func, data_chunks, *args, **kwargs)
    
    def _check_memory_availability(self) -> bool:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        memory = psutil.virtual_memory()
        available_ratio = memory.available / memory.total
        return available_ratio > (1 - self.max_memory_usage)
    
    def optimize_dataframe_operations(self, df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æ“ä½œæœ€é©åŒ–"""
        
        if PROFESSIONAL_LOGGING:
            professional_logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æœ€é©åŒ–é–‹å§‹", shape=df.shape)
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–
        optimized_df = self._optimize_memory_usage(df.copy())
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®æœ€é©åŒ–
        optimized_df = self._optimize_categorical_columns(optimized_df)
        
        # æ¬ æå€¤å‡¦ç†ã®æœ€é©åŒ–
        optimized_df = self._optimize_missing_values(optimized_df)
        
        if PROFESSIONAL_LOGGING:
            memory_saved = df.memory_usage(deep=True).sum() - optimized_df.memory_usage(deep=True).sum()
            professional_logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æœ€é©åŒ–å®Œäº†", 
                                   memory_saved_mb=memory_saved / (1024**2))
        
        return optimized_df
    
    def _optimize_memory_usage(self, df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–"""
        
        for col in df.columns:
            if df[col].dtype == 'object':
                continue
                
            # æ•´æ•°å‹ã®æœ€é©åŒ–
            if df[col].dtype.kind in 'biufc':
                if df[col].dtype == 'int64':
                    if df[col].min() >= -128 and df[col].max() <= 127:
                        df[col] = df[col].astype('int8')
                    elif df[col].min() >= -32768 and df[col].max() <= 32767:
                        df[col] = df[col].astype('int16')
                    elif df[col].min() >= -2147483648 and df[col].max() <= 2147483647:
                        df[col] = df[col].astype('int32')
                
                # æµ®å‹•å°æ•°ç‚¹å‹ã®æœ€é©åŒ–
                elif df[col].dtype == 'float64':
                    df[col] = pd.to_numeric(df[col], downcast='float')
        
        return df
    
    def _optimize_categorical_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°æœ€é©åŒ–"""
        
        for col in df.select_dtypes(include=['object']).columns:
            if df[col].nunique() / len(df) < 0.5:  # 50%æœªæº€ãŒãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå ´åˆ
                df[col] = df[col].astype('category')
        
        return df
    
    def _optimize_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¬ æå€¤å‡¦ç†æœ€é©åŒ–"""
        
        # ã‚¹ãƒ‘ãƒ¼ã‚¹é…åˆ—ã®æ´»ç”¨
        for col in df.columns:
            if df[col].isnull().sum() > len(df) * 0.9:  # 90%ä»¥ä¸ŠãŒæ¬ æ
                try:
                    df[col] = df[col].astype(pd.SparseDtype(df[col].dtype))
                except:
                    pass  # ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã‚‚ç¶šè¡Œ
        
        return df

class OptimizedStatistics:
    """æœ€é©åŒ–ã•ã‚ŒãŸçµ±è¨ˆè¨ˆç®—"""
    
    def __init__(self, use_numba: bool = NUMBA_AVAILABLE):
        self.use_numba = use_numba and NUMBA_AVAILABLE
        
        if PROFESSIONAL_LOGGING:
            professional_logger.info("æœ€é©åŒ–çµ±è¨ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–", use_numba=self.use_numba)
    
    def fast_correlation_matrix(self, data: np.ndarray) -> np.ndarray:
        """é«˜é€Ÿç›¸é–¢è¡Œåˆ—è¨ˆç®—"""
        if self.use_numba:
            return self._fast_correlation_matrix_numba(data)
        else:
            return np.corrcoef(data.T)
    
    def _fast_correlation_matrix_numba(self, data: np.ndarray) -> np.ndarray:
        """Numbaæœ€é©åŒ–ç›¸é–¢è¡Œåˆ—è¨ˆç®—"""
        if not NUMBA_AVAILABLE:
            return np.corrcoef(data.T)
        
        @jit(nopython=True, parallel=True)
        def _compute_correlation(data):
            n_vars = data.shape[1]
            corr_matrix = np.zeros((n_vars, n_vars))
            
            # å¹³å‡ã®è¨ˆç®—
            means = np.mean(data, axis=0)
            
            # ç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—
            for i in prange(n_vars):
                for j in prange(i, n_vars):
                    if i == j:
                        corr_matrix[i, j] = 1.0
                    else:
                        # å…±åˆ†æ•£ã®è¨ˆç®—
                        cov = np.mean((data[:, i] - means[i]) * (data[:, j] - means[j]))
                        # æ¨™æº–åå·®ã®è¨ˆç®—
                        std_i = np.sqrt(np.mean((data[:, i] - means[i]) ** 2))
                        std_j = np.sqrt(np.mean((data[:, j] - means[j]) ** 2))
                        
                        if std_i > 0 and std_j > 0:
                            corr_matrix[i, j] = cov / (std_i * std_j)
                            corr_matrix[j, i] = corr_matrix[i, j]
                        else:
                            corr_matrix[i, j] = 0.0
                            corr_matrix[j, i] = 0.0
            
            return corr_matrix
        
        return _compute_correlation(data)
    
    def fast_percentile(self, data: np.ndarray, percentile: float) -> float:
        """é«˜é€Ÿãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—"""
        if self.use_numba:
            return self._fast_percentile_numba(data, percentile)
        else:
            return np.percentile(data, percentile)
    
    def _fast_percentile_numba(self, data: np.ndarray, percentile: float) -> float:
        """Numbaæœ€é©åŒ–ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—"""
        if not NUMBA_AVAILABLE:
            return np.percentile(data, percentile)
        
        @jit(nopython=True)
        def _compute_percentile(data, percentile):
            sorted_data = np.sort(data.flatten())
            n = len(sorted_data)
            index = (n - 1) * percentile / 100.0
            
            if index == int(index):
                return sorted_data[int(index)]
            else:
                lower = sorted_data[int(index)]
                upper = sorted_data[int(index) + 1]
                return lower + (upper - lower) * (index - int(index))
        
        return _compute_percentile(data, percentile)
    
    def compute_descriptive_statistics(self, data: np.ndarray) -> Dict[str, float]:
        """æœ€é©åŒ–ã•ã‚ŒãŸè¨˜è¿°çµ±è¨ˆè¨ˆç®—"""
        
        if self.use_numba:
            return self._compute_stats_numba(data)
        else:
            return self._compute_stats_numpy(data)
    
    def _compute_stats_numba(self, data: np.ndarray) -> Dict[str, float]:
        """Numbaæœ€é©åŒ–çµ±è¨ˆè¨ˆç®—"""
        if not NUMBA_AVAILABLE:
            return self._compute_stats_numpy(data)
        
        @jit(nopython=True)
        def _compute_stats(data):
            flat_data = data.flatten()
            n = len(flat_data)
            
            # åŸºæœ¬çµ±è¨ˆ
            mean = np.mean(flat_data)
            std = np.std(flat_data)
            var = np.var(flat_data)
            min_val = np.min(flat_data)
            max_val = np.max(flat_data)
            
            # æ­ªåº¦ãƒ»å°–åº¦ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            centered = flat_data - mean
            skewness = np.mean(centered**3) / (std**3) if std > 0 else 0.0
            kurtosis = np.mean(centered**4) / (std**4) - 3.0 if std > 0 else 0.0
            
            return mean, std, var, min_val, max_val, skewness, kurtosis, float(n)
        
        mean, std, var, min_val, max_val, skewness, kurtosis, count = _compute_stats(data)
        
        return {
            'mean': mean,
            'std': std,
            'var': var,
            'min': min_val,
            'max': max_val,
            'skewness': skewness,
            'kurtosis': kurtosis,
            'count': count
        }
    
    def _compute_stats_numpy(self, data: np.ndarray) -> Dict[str, float]:
        """NumPyçµ±è¨ˆè¨ˆç®—"""
        flat_data = data.flatten()
        
        return {
            'mean': np.mean(flat_data),
            'std': np.std(flat_data),
            'var': np.var(flat_data),
            'min': np.min(flat_data),
            'max': np.max(flat_data),
            'median': np.median(flat_data),
            'q25': np.percentile(flat_data, 25),
            'q75': np.percentile(flat_data, 75),
            'skewness': float(pd.Series(flat_data).skew()),
            'kurtosis': float(pd.Series(flat_data).kurtosis()),
            'count': float(len(flat_data))
        }

class MemoryManager:
    """ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, max_memory_percent: float = 80.0):
        self.max_memory_percent = max_memory_percent
        self.memory_warnings = []
        
        if PROFESSIONAL_LOGGING:
            professional_logger.info("ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–", 
                                   max_memory_percent=max_memory_percent)
    
    def monitor_memory(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªç›£è¦–"""
        memory = psutil.virtual_memory()
        
        memory_info = {
            'total_gb': memory.total / (1024**3),
            'available_gb': memory.available / (1024**3),
            'used_gb': memory.used / (1024**3),
            'percent_used': memory.percent,
            'is_critical': memory.percent > self.max_memory_percent
        }
        
        if memory_info['is_critical']:
            self.memory_warnings.append({
                'timestamp': datetime.now(),
                'memory_percent': memory.percent,
                'message': f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒ{memory.percent:.1f}%ã«é”ã—ã¾ã—ãŸ"
            })
            
            if PROFESSIONAL_LOGGING:
                professional_logger.warning("ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡è­¦å‘Š", 
                                          memory_percent=memory.percent)
        
        return memory_info
    
    def cleanup_memory(self):
        """ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if PROFESSIONAL_LOGGING:
            professional_logger.info("ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ")
        
        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å¼·åˆ¶å®Ÿè¡Œ
        collected = gc.collect()
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ
        if hasattr(gc, 'set_threshold'):
            gc.set_threshold(700, 10, 10)
        
        return {
            'objects_collected': collected,
            'memory_after_cleanup': psutil.virtual_memory().percent
        }
    
    def get_memory_recommendations(self) -> List[str]:
        """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æ¨å¥¨äº‹é …"""
        recommendations = []
        memory = psutil.virtual_memory()
        
        if memory.percent > 90:
            recommendations.append("ğŸš¨ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒ90%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦å‡¦ç†ã—ã¦ãã ã•ã„")
        elif memory.percent > 80:
            recommendations.append("âš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒ80%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ä¸è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ãã ã•ã„")
        elif memory.percent > 70:
            recommendations.append("ğŸ’¡ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒ70%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        
        if len(self.memory_warnings) > 5:
            recommendations.append("ğŸ“Š é »ç¹ã«ãƒ¡ãƒ¢ãƒªè­¦å‘ŠãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ–¹æ³•ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„")
        
        return recommendations

class BatchProcessor:
    """ãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, batch_size: int = 1000, progress_callback: Callable = None):
        self.batch_size = batch_size
        self.progress_callback = progress_callback
        self.processed_batches = 0
        self.failed_batches = 0
        
        if PROFESSIONAL_LOGGING:
            professional_logger.info("ãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–", batch_size=batch_size)
    
    def process_in_batches(self, data: pd.DataFrame, 
                          processing_func: Callable,
                          *args, **kwargs) -> List[Any]:
        """ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ"""
        
        total_rows = len(data)
        num_batches = (total_rows + self.batch_size - 1) // self.batch_size
        results = []
        
        if PROFESSIONAL_LOGGING:
            professional_logger.info("ãƒãƒƒãƒå‡¦ç†é–‹å§‹", 
                                   total_rows=total_rows, 
                                   num_batches=num_batches)
        
        with tqdm(total=num_batches, desc="ğŸ“¦ Batch Processing") as pbar:
            for i in range(0, total_rows, self.batch_size):
                batch_data = data.iloc[i:i + self.batch_size]
                
                try:
                    batch_result = processing_func(batch_data, *args, **kwargs)
                    results.append(batch_result)
                    self.processed_batches += 1
                    
                    if self.progress_callback:
                        self.progress_callback(i + len(batch_data), total_rows)
                
                except Exception as e:
                    self.failed_batches += 1
                    if PROFESSIONAL_LOGGING:
                        professional_logger.error(f"ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ (batch {i//self.batch_size}): {e}")
                    results.append(None)
                
                pbar.update(1)
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
                if psutil.virtual_memory().percent > 85:
                    gc.collect()
        
        success_rate = (self.processed_batches / num_batches) * 100 if num_batches > 0 else 0
        
        if PROFESSIONAL_LOGGING:
            professional_logger.info("ãƒãƒƒãƒå‡¦ç†å®Œäº†", 
                                   success_rate=success_rate,
                                   failed_batches=self.failed_batches)
        
        return results
    
    def get_batch_statistics(self) -> Dict[str, Any]:
        """ãƒãƒƒãƒå‡¦ç†çµ±è¨ˆ"""
        total_batches = self.processed_batches + self.failed_batches
        
        return {
            'total_batches': total_batches,
            'successful_batches': self.processed_batches,
            'failed_batches': self.failed_batches,
            'success_rate': (self.processed_batches / total_batches * 100) if total_batches > 0 else 0,
            'current_batch_size': self.batch_size
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
parallel_processor = ParallelProcessor(backend='threading')
optimized_stats = OptimizedStatistics()
memory_manager = MemoryManager()
batch_processor = BatchProcessor()